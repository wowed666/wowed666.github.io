---
layout: post
title: Blog Post 3 Fake News Detector
---


In this blog we are going to build a Fake News Detector model by machine learning.

# Data source
Our data for this tutorial comes from the article
- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).


## Import Modules And Acquire Training Data

Firstly let's import the modules we will need!

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Then let's Acquire Training Data! And take a peek.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
There are four columns in our dataframe. 
- "0" column looks like nonsense we can just skip.
- "title" column is the title of the news.
- "text" column is the content of news.
- "fake" column let us know whether this news is fake.("0" present it is not fake news; "1" present it is fake news!)

## Make a Dataset

Next step we gonna make a dataset!
We want to define a function able to do two things:
 - Remove the stopwords from "text" and "title".
 - Construct and return a tf.data.Dataset with two inputs and one output

Stopwords are uninformative words such as("and","but"...)
We could import stopwords form the module "nltk"

```python
from nltk.corpus import stopwords
stop = stopwords.words('english')

print(stop)
```
```
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
```
It is a list of all stopwords.

Then let's define our 'make_dataset()' function!
If you are not familiar with the text clean. Here is the link of tutorial of how to clean text 
<a href="https://machinelearningmastery.com/clean-text-machine-learning-python/#:~:text=Split%20by%20Whitespace%20and%20Remove%20Punctuation&text=One%20way%20would%20be%20to,nothing%20(e.g.%20remove%20it).&text=Python%20offers%20a%20function%20called,set%20of%20characters%20to%20another.">Click Me!</a>

Before we return the dataset we should batch it. Batching causes our model to train on trunks of data instead of individual rows. Sometimes it would reduce the accuracy, but it also can increase the speed of training.
The key is find the balance! I found batches of 100 rows to work well.
```python
def make_dataset(df):
    """
    This function do : 1.remove the stopwords from dataframe.
                       2.return a tf.data.Dataset
    input: dataframe
    output: tf.data.Dataset without stopwords
    """
    #first remove stopwords
    #get the stop words
    stop = stopwords.words('english')
    
    for i in ["title", "text"]:
        df[i] = df[i].apply(lambda x: " ".join([i for i in re.split(r'\W+', x.lower()) if i not in stop]))
    # re.split(r'\W+', x.lower()) allow us to split the string and make it lowercase without punctuations
    
    
    # construct tf.data.Dataset
    dataset =tf.data.Dataset.from_tensor_slices(
        (
            {
                "title" : df[["title"]],
                "text" : df[["text"]]
            }, 
            {
                "fake" : df[["fake"]]
            }
        )
    ) 
    
    
    
    return dataset
```
Let's  split of 20% of dataset to use for validation and 10% for test dataset.


```python

Dataset = make_dataset(df)
Dataset = Dataset.shuffle(buffer_size = len(Dataset))

train_size = int(0.7*len(Dataset))
val_size   = int(0.2*len(Dataset))

train = Dataset.take(train_size).batch(100)
val   = Dataset.skip(train_size).take(val_size).batch(100)
test  = Dataset.skip(train_size + val_size).batch(100)

len(train), len(val), len(test)
```
```
(158, 45, 23)
```