---
layout: post
title: Blog Post 3 Fake News Detector
---


In this blog we are going to build a Fake News Detector model by machine learning.

# Data source
Our data for this tutorial comes from the article
- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).


## Import Modules And Acquire Training Data

Firstly let's import the modules we will need!

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Then let's Acquire Training Data! And take a peek.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
There are four columns in our dataframe. 
- "0" column looks like nonsense we can just skip.
- "title" column is the title of the news.
- "text" column is the content of news.
- "fake" column let us know whether this news is fake.("0" present it is not fake news; "1" present it is fake news!)

## Make a Dataset

Next step we gonna make a dataset!
We want to define a function able to do two things:
 - Remove the stopwords from "text" and "title".
 - Construct and return a tf.data.Dataset with two inputs and one output

Stopwords are uninformative words such as("and","but"...)
We could import stopwords form the module "nltk"

```python
from nltk.corpus import stopwords
stop = stopwords.words('english')

print(stop)
```
```
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
```
It is a list of all stopwords.

Then let's define our 'make_dataset()' function!
If you are not familiar with the text clean. Here is the link of tutorial of how to clean text 
<a href="https://machinelearningmastery.com/clean-text-machine-learning-python/#:~:text=Split%20by%20Whitespace%20and%20Remove%20Punctuation&text=One%20way%20would%20be%20to,nothing%20(e.g.%20remove%20it).&text=Python%20offers%20a%20function%20called,set%20of%20characters%20to%20another.">Click Me!</a>

Before we return the dataset we should batch it. Batching causes our model to train on trunks of data instead of individual rows. Sometimes it would reduce the accuracy, but it also can increase the speed of training.
The key is find the balance! I found batches of 100 rows to work well.
```python
def make_dataset(df):
    """
    This function do : 1.remove the stopwords from dataframe.
                       2.return a tf.data.Dataset
    input: dataframe
    output: tf.data.Dataset without stopwords
    """
    #first remove stopwords
    #get the stop words
    stop = stopwords.words('english')
    
    for i in ["title", "text"]:
        df[i] = df[i].apply(lambda x: " ".join([i for i in re.split(r'\W+', x.lower()) if i not in stop]))
    # re.split(r'\W+', x.lower()) allow us to split the string and make it lowercase without punctuations
    
    
    # construct tf.data.Dataset
    dataset =tf.data.Dataset.from_tensor_slices(
        (
            {
                "title" : df[["title"]],
                "text" : df[["text"]]
            }, 
            {
                "fake" : df[["fake"]]
            }
        )
    ) 
    
    
    
    return dataset
```
Let's  split of 20% of dataset to use for validation and 10% for test dataset.


```python

Dataset = Dataset.shuffle(buffer_size = len(Dataset))


val_size   = int(0.2*len(Dataset))

val   = Dataset.take(val_size)
train = Dataset.skip(val_size)


len(train), len(val)
```
```
(180, 45)
```

# Creat Models

We have both data of article title and article text. What is the most effective way? Only focus on title or text or both?
Let's creat three models and compare their accuracy!
- In the first model,we use only the article title as an input.
- In the second model,we use only the article text as an input.
- In the third model, we should use both the article title and the article text as input

Before we construct our models we should do <b>Standardization</b> and <b>Vectorization</b> with our data!

<b>Standardization</b> refer to make our data "clean":
 - Remove captials.
 - Remove punctuation.
 - Remove HTML elements or other non-semantic content.

 Scince we already do this in our make_dataset function we don't need do any thing more. yeah!!!

<b>Vectorization</b> refers to the process of representing text as a vector (array, tensor). There are different ways to do this, but for this model we'll replace each word by its frequency rank in the data.

```python
# the maximum ranking 
max_tokens = 2000

def vectorize_layer(ds,col,squ_len):
    """
    Inputs: 
        ds : dataset we want to vectorize
        col: column name we want to vectorize
        squ_len: output sequence length

    output: vectorization_layer

    """
    vec_layer = TextVectorization(
        max_tokens  = max_tokens,
        output_mode='int',
        output_sequence_length = squ_len) 

    vec_layer.adapt(train.map(lambda x, y: x[col]))
    return vec_layer
```

```python
title_vec_layer = vectorize_layer(train, 'title',100)
text_vec_layer = vectorize_layer(train, 'text',500)
```
define the input of models
```python
title_input = keras.Input(shape=(1,), name='title', dtype='string')
text_input = keras.Input(shape=(1,), name='text', dtype='string')
```
We want text_model and title_model share embedding
```python
shared_embedding = layers.Embedding(max_tokens, 30, name = 'embedding')
```

Since we want to creat two similar models. Let's  write a function to set the features
```python
def set_features(col_input,col_name, vec_layer):
    
    features = vec_layer(col_input)
    # Embedding layer present a word in a vector space. Each word is assigned an individual vector.
    # Words with related meanings are close to each other in a vector space, while words with different meanings are farther apart.
    features = layers.Embedding(max_tokens, 30, name = col_name + "_embedding")(features)
    # layers.Dropout would encounter overfitting
    features = layers.Dropout(0.2)(features)
    # Pooling layer would reduce the size of data at each step
    features = layers.GlobalAveragePooling1D()(features) 
    features = layers.Dropout(0.2)(features) 
    features = layers.Dense(32, activation='relu')(features)
    
    return features
```

```python
title_features = set_features(title_input,"title",title_vec_layer)
text_features = set_features(text_input,"text",text_vec_layer)
```

Let's set the output!
```python
title_output = layers.Dense(2, name = "fake")(title_features)
text_output = layers.Dense(2, name = "fake")(text_features)
```
Now we have both input and output for our model. It's time to build models!

```python
title_model = keras.Model(
    inputs = title_input,
    outputs = title_output
)

title_model.summary()
```
<pre>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
title (InputLayer)           [(None, 1)]               0         
_________________________________________________________________
text_vectorization_2 (TextVe (None, 100)               0         
_________________________________________________________________
embedding (Embedding)        multiple                  60000   
_________________________________________________________________
dropout (Dropout)            (None, 100, 30)           0         
_________________________________________________________________
global_average_pooling1d (Gl (None, 30)                0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 30)                0         
_________________________________________________________________
dense (Dense)                (None, 32)                992       
_________________________________________________________________
fake (Dense)                 (None, 2)                 66        
=================================================================
Total params: 61,058
Trainable params: 61,058
Non-trainable params: 0
_________________________________________________________________
</pre>
Here we go! Now we can see the structure of our model!

```python
keras.utils.plot_model(title_model)
```
![title_model_plot.png](/images/Blog3/title_model_plot.png)

Let's compile title_model!
```python
title_model.compile(optimizer = "adam",
                    loss=losses.SparseCategoricalCrossentropy(from_logits=True),
                    metrics=['accuracy'])
```

Now we can fit title_model! 
```python
title_history = title_model.fit(train, epochs = 10, validation_data = val)
```
<pre>Epoch 1/10
180/180 [==============================] - 2s 9ms/step - loss: 0.6845 - accuracy: 0.5626 - val_loss: 0.5465 - val_accuracy: 0.8567
Epoch 2/10
180/180 [==============================] - 1s 7ms/step - loss: 0.4490 - accuracy: 0.8643 - val_loss: 0.2468 - val_accuracy: 0.9133
Epoch 3/10
180/180 [==============================] - 1s 7ms/step - loss: 0.2267 - accuracy: 0.9197 - val_loss: 0.1694 - val_accuracy: 0.9384
Epoch 4/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1718 - accuracy: 0.9345 - val_loss: 0.1403 - val_accuracy: 0.9480
Epoch 5/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1521 - accuracy: 0.9417 - val_loss: 0.1427 - val_accuracy: 0.9440
Epoch 6/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1392 - accuracy: 0.9453 - val_loss: 0.1205 - val_accuracy: 0.9544
Epoch 7/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1262 - accuracy: 0.9523 - val_loss: 0.1199 - val_accuracy: 0.9520
Epoch 8/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1220 - accuracy: 0.9517 - val_loss: 0.1042 - val_accuracy: 0.9587
Epoch 9/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1116 - accuracy: 0.9577 - val_loss: 0.1080 - val_accuracy: 0.9560
Epoch 10/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1144 - accuracy: 0.9583 - val_loss: 0.0986 - val_accuracy: 0.9647
</pre>
Let's visualize the change of accuracy during each Epoch.
```python
from matplotlib import pyplot as plt
plt.plot(title_history.history["accuracy"], label = "training")
plt.plot(title_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![hist_plot1.png](/images/Blog3/hist_plot1.png)

Our first model looks like very good! The validation accuracy is over 95%!

Let's creat the second model! 
```python
text_model = keras.Model(
    inputs = text_input,
    outputs = text_output
)

text_model.summary()
```
<pre>Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
text (InputLayer)            [(None, 1)]               0         
_________________________________________________________________
text_vectorization_3 (TextVe (None, 500)               0         
_________________________________________________________________
embedding (Embedding)        multiple                  60000     
_________________________________________________________________
dropout_2 (Dropout)          (None, 500, 30)           0         
_________________________________________________________________
global_average_pooling1d_1 ( (None, 30)                0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 30)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                992       
_________________________________________________________________
fake (Dense)                 (None, 2)                 66        
=================================================================
Total params: 61,058
Trainable params: 61,058
Non-trainable params: 0
_________________________________________________________________
</pre>

```python
keras.utils.plot_model(text_model)
```
![text_model_plot.png](/images/Blog3/text_model_plot.png)

```python
text_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

text_history = text_model.fit(train, epochs = 10, validation_data = val)
```
<pre>Epoch 1/10
180/180 [==============================] - 7s 34ms/step - loss: 0.6540 - accuracy: 0.6033 - val_loss: 0.3170 - val_accuracy: 0.9431
Epoch 2/10
180/180 [==============================] - 6s 33ms/step - loss: 0.2584 - accuracy: 0.9314 - val_loss: 0.1583 - val_accuracy: 0.9498
Epoch 3/10
180/180 [==============================] - 6s 33ms/step - loss: 0.1455 - accuracy: 0.9613 - val_loss: 0.1188 - val_accuracy: 0.9699
Epoch 4/10
180/180 [==============================] - 6s 33ms/step - loss: 0.1170 - accuracy: 0.9687 - val_loss: 0.0974 - val_accuracy: 0.9800
Epoch 5/10
180/180 [==============================] - 6s 33ms/step - loss: 0.0941 - accuracy: 0.9735 - val_loss: 0.0862 - val_accuracy: 0.9796
Epoch 6/10
180/180 [==============================] - 6s 34ms/step - loss: 0.0863 - accuracy: 0.9777 - val_loss: 0.0731 - val_accuracy: 0.9822
Epoch 7/10
180/180 [==============================] - 6s 33ms/step - loss: 0.0723 - accuracy: 0.9821 - val_loss: 0.0669 - val_accuracy: 0.9827
Epoch 8/10
180/180 [==============================] - 6s 33ms/step - loss: 0.0622 - accuracy: 0.9850 - val_loss: 0.0501 - val_accuracy: 0.9885
Epoch 9/10
180/180 [==============================] - 6s 34ms/step - loss: 0.0579 - accuracy: 0.9870 - val_loss: 0.0525 - val_accuracy: 0.9872
Epoch 10/10
180/180 [==============================] - 6s 33ms/step - loss: 0.0567 - accuracy: 0.9876 - val_loss: 0.0470 - val_accuracy: 0.9902
</pre>

```python
plt.plot(text_history.history["accuracy"], label = "training")
plt.plot(text_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![hist_plot2.png](/images/Blog3/hist_plot2.png)

Both models doing very good,let's try to combine those two models and see if it will do better!

```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation='relu')(main) 
output = layers.Dense(2, name = "fake")(main)

main_model = keras.Model(
    inputs = [title_input, text_input], 
    outputs = output
)

main_model.summary()
```
<pre>Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
title (InputLayer)              [(None, 1)]          0                                            
__________________________________________________________________________________________________
text (InputLayer)               [(None, 1)]          0                                            
__________________________________________________________________________________________________
text_vectorization_2 (TextVecto (None, 100)          0           title[0][0]                      
__________________________________________________________________________________________________
text_vectorization_3 (TextVecto (None, 500)          0           text[0][0]                       
__________________________________________________________________________________________________
embedding (Embedding)           multiple             60000       text_vectorization_2[2][0]       
                                                                 text_vectorization_3[1][0]       
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 100, 30)      0           embedding[0][0]                  
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 500, 30)      0           embedding[1][0]                  
__________________________________________________________________________________________________
global_average_pooling1d_2 (Glo (None, 30)           0           dropout_4[0][0]                  
__________________________________________________________________________________________________
global_average_pooling1d_3 (Glo (None, 30)           0           dropout_6[0][0]                  
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 30)           0           global_average_pooling1d_2[0][0] 
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 30)           0           global_average_pooling1d_3[0][0] 
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 32)           992         dropout_5[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 32)           992         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 64)           0           dense_3[0][0]                    
                                                                 dense_4[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 32)           2080        concatenate_2[0][0]              
__________________________________________________________________________________________________
fake (Dense)                    (None, 2)            66          dense_6[0][0]                    
==================================================================================================
Total params: 64,130
Trainable params: 64,130
Non-trainable params: 0
__________________________________________________________________________________________________
</pre>

```python
keras.utils.plot_model(main_model)
```
![main_model_plot.png](/images/Blog3/main_model_plot.png)


```python
main_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=['accuracy']
)
```

```python
main_history = main_model.fit(train, 
                              validation_data=val,
                              epochs = 10)
```
<pre>Epoch 1/10
180/180 [==============================] - 9s 42ms/step - loss: 0.2621 - accuracy: 0.9406 - val_loss: 0.0263 - val_accuracy: 0.9951
Epoch 2/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0262 - accuracy: 0.9922 - val_loss: 0.0175 - val_accuracy: 0.9953
Epoch 3/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0117 - val_accuracy: 0.9973
Epoch 4/10
180/180 [==============================] - 7s 41ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 0.0119 - val_accuracy: 0.9964
Epoch 5/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.0054 - val_accuracy: 0.9984
Epoch 6/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.0077 - val_accuracy: 0.9987
Epoch 7/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0086 - val_accuracy: 0.9978
Epoch 8/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0063 - val_accuracy: 0.9989
Epoch 9/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0043 - val_accuracy: 0.9989
Epoch 10/10
180/180 [==============================] - 7s 38ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0035 - val_accuracy: 0.9993
</pre>

```python
plt.plot(main_history.history["accuracy"], label = "training")
plt.plot(main_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![hist_plot3.png](/images/Blog3/hist_plot3.png)

# Model Evaluation

Our main_model doing very well! Let's use test data challenge our model.
```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
main_model.evaluate(test_Dataset)
```
<pre>225/225 [==============================] - 3s 13ms/step - loss: 0.0430 - accuracy: 0.9887
</pre>
[0.043015770614147186, 0.9886854887008667]

Our main_model facing test data reached 98.8% accuracy!!! Good job!


# Embedding Visualization

```python
from sklearn.decomposition import PCA
import plotly.express as px 


weights = main_model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = title_vec_layer.get_vocabulary() 

pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
'word' : vocab, 
'x0'   : weights[:,0],
'x1'   : weights[:,1]
})

embedding_df
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.120822</td>
      <td>-0.201211</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.074590</td>
      <td>-0.137953</td>
    </tr>
    <tr>
      <th>2</th>
      <td>trump</td>
      <td>-0.048714</td>
      <td>0.147146</td>
    </tr>
    <tr>
      <th>3</th>
      <td>video</td>
      <td>-2.647133</td>
      <td>3.970165</td>
    </tr>
    <tr>
      <th>4</th>
      <td>u</td>
      <td>-0.187628</td>
      <td>-1.571342</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>unveils</td>
      <td>-0.340787</td>
      <td>-1.445624</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>toward</td>
      <td>-2.487086</td>
      <td>-1.010876</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>thai</td>
      <td>-1.196269</td>
      <td>-1.760851</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>tea</td>
      <td>0.122441</td>
      <td>1.007159</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>targeted</td>
      <td>0.073817</td>
      <td>0.071684</td>
    </tr>
  </tbody>
</table>

```python
vocab2 = text_vec_layer.get_vocabulary() 
embedding_df2 = pd.DataFrame({
'word' : vocab, 
'x0'   : weights[:,0],
'x1'   : weights[:,1]
})
embedding_df2

```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.120822</td>
      <td>-0.201211</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.074590</td>
      <td>-0.137953</td>
    </tr>
    <tr>
      <th>2</th>
      <td>trump</td>
      <td>-0.048714</td>
      <td>0.147146</td>
    </tr>
    <tr>
      <th>3</th>
      <td>video</td>
      <td>-2.647133</td>
      <td>3.970165</td>
    </tr>
    <tr>
      <th>4</th>
      <td>u</td>
      <td>-0.187628</td>
      <td>-1.571342</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>unveils</td>
      <td>-0.340787</td>
      <td>-1.445624</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>toward</td>
      <td>-2.487086</td>
      <td>-1.010876</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>thai</td>
      <td>-1.196269</td>
      <td>-1.760851</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>tea</td>
      <td>0.122441</td>
      <td>1.007159</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>targeted</td>
      <td>0.073817</td>
      <td>0.071684</td>
    </tr>
  </tbody>
</table>

We observed embedding_df2 and embedding_df is same because they shared embedding

```python
fig = px.scatter(embedding_df, 
             x = "x0", 
             y = "x1", 
             size = list(np.ones(len(embedding_df))),
             size_max = 2,
             hover_name = "word")

fig.show()
```

{% include emb_scatter.html %}




